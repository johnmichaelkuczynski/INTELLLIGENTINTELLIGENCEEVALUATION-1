import { Response } from 'express';

const EXACT_COMPLETE_QUESTIONS = `IS IT INSIGHTFUL? 
DOES IT DEVELOP POINTS? (OR, IF IT IS A SHORT EXCERPT, IS THERE EVIDENCE THAT IT WOULD DEVELOP POINTS IF EXTENDED)? 
IS THE ORGANIZATION MERELY SEQUENTIAL (JUST ONE POINT AFTER ANOTHER, LITTLE OR NO LOGICAL SCAFFOLDING)? OR ARE THE IDEAS ARRANGED, NOT JUST SEQUENTIALLY BUT HIERARCHICALLY? 
IF THE POINTS IT MAKES ARE NOT INSIGHTFUL, DOES IT OPERATE SKILLFULLY WITH CANONS OF LOGIC/REASONING. 
ARE THE POINTS CLICHES? OR ARE THEY "FRESH"? 
DOES IT USE TECHNICAL JARGON TO OBFUSCATE OR TO RENDER MORE PRECISE? 
IS IT ORGANIC? DO POINTS DEVELOP IN AN ORGANIC, NATURAL WAY? DO THEY 'UNFOLD'? OR ARE THEY FORCED AND ARTIFICIAL? 
DOES IT OPEN UP NEW DOMAINS? OR, ON THE CONTRARY, DOES IT SHUT OFF INQUIRY (BY CONDITIONALIZING FURTHER DISCUSSION OF THE MATTERS ON ACCEPTANCE OF ITS INTERNAL AND POSSIBLY VERY FAULTY LOGIC)? 
IS IT ACTUALLY INTELLIGENT OR JUST THE WORK OF SOMEBODY WHO, JUDGING BY THE SUBJECT-MATTER, IS PRESUMED TO BE INTELLIGENT (BUT MAY NOT BE)? 
IS IT REAL OR IS IT PHONY? 
DO THE SENTENCES EXHIBIT COMPLEX AND COHERENT INTERNAL LOGIC? 
IS THE PASSAGE GOVERNED BY A STRONG CONCEPT? OR IS THE ONLY ORGANIZATION DRIVEN PURELY BY EXPOSITORY (AS OPPOSED TO EPISTEMIC) NORMS?
IS THERE SYSTEM-LEVEL CONTROL OVER IDEAS? IN OTHER WORDS, DOES THE AUTHOR SEEM TO RECALL WHAT HE SAID EARLIER AND TO BE IN A POSITION TO INTEGRATE IT INTO POINTS HE HAS MADE SINCE THEN? 
ARE THE POINTS 'REAL'? ARE THEY FRESH? OR IS SOME INSTITUTION OR SOME ACCEPTED VEIN OF PROPAGANDA OR ORTHODOXY JUST USING THE AUTHOR AS A MOUTH PIECE?
IS THE WRITING EVASIVE OR DIRECT? 
ARE THE STATEMENTS AMBIGUOUS? 
DOES THE PROGRESSION OF THE TEXT DEVELOP ACCORDING TO WHO SAID WHAT OR ACCORDING TO WHAT ENTAILS OR CONFIRMS WHAT? 
DOES THE AUTHOR USE OTHER AUTHORS TO DEVELOP HIS IDEAS OR TO CLOAK HIS OWN LACK OF IDEAS?

ADDITIONAL CRITICAL QUESTIONS:
ARE THERE TERMS THAT ARE UNDEFINED BUT SHOULD BE DEFINED, IN THE SENSE THAT, WITHOUT DEFINITIONS, IT IS DIFFICULT OR IMPOSSIBLE TO KNOW WHAT IS BEING SAID OR THEREFORE TO EVALUATE WHAT IS BEING SAID?
ARE THERE "FREE VARIABLES" IN THE TEXT? IE ARE THERE QUALIFICATIONS OR POINTS THAT ARE MADE BUT DO NOT CONNECT TO ANYTHING LATER OR EARLIER?
DO NEW STATEMENTS DEVELOP OUT OF OLD ONES? OR ARE THEY MERELY "ADDED" TO PREVIOUS ONES, WITHOUT IN ANY SENSE BEING GENERATED BY THEM?
DO NEW STATEMENTS CLARIFY OR DO THEY LEAD TO MORE LACK OF CLARITY?
IS THE PASSAGE ACTUALLY (PALPABLY) SMART? OR IS ONLY "PRESUMPTION-SMART"?
IF YOUR JUDGMENT IS THAT IT IS INSIGHTFUL, CAN YOU STATEMENT THAT INSIGHT IN A SINGLE SENTENCE?
HOW WELL DOES IT MAKE ITS CASE?
IF I WERE TO GIVE A HIGH SCORE TO THIS PASSAGE, WOULD I BE REWARDING IMPOSTOR SCAFFOLDING?
IF I WERE TO GIVE A HIGH SCORE TO THIS PASSAGE, WOULD I BE REWARDING CONFORMITY TO ACADEMIC/BUREAUCRATIC NORMS?
IF I WERE TO GIVE A LOW SCORE TO THIS PASSAGE, WOULD I BE PENALIZING ACTUAL INTELLIGENCE OWING TO A LACK OF CONFORMITY TO ACADEMIC/BUREAUCRATIC NORMS?`;

// Generic LLM caller
async function callLLMProvider(
  provider: 'openai' | 'anthropic' | 'deepseek',
  messages: Array<{role: string, content: string}>
): Promise<string> {
  try {
    console.log(`CALLING ${provider.toUpperCase()} with prompt length: ${messages[0]?.content?.length || 0}`);
    
    if (provider === 'openai') {
      const OpenAI = (await import('openai')).default;
      const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
      
      const completion = await openai.chat.completions.create({
        model: "gpt-4o",
        messages: messages as any,
        temperature: 0.1,
        max_tokens: 4000
      });
      
      const result = completion.choices?.[0]?.message?.content || '';
      console.log(`${provider.toUpperCase()} RESPONSE LENGTH: ${result.length}`);
      console.log(`${provider.toUpperCase()} RESPONSE PREVIEW: "${result.substring(0, 200)}..."`);
      return result;
    } else if (provider === 'anthropic') {
      const response = await fetch('https://api.anthropic.com/v1/messages', {
        method: 'POST',
        headers: {
          'x-api-key': process.env.ANTHROPIC_API_KEY!,
          'Content-Type': 'application/json',
          'anthropic-version': '2023-06-01'
        },
        body: JSON.stringify({
          model: "claude-3-sonnet-20240229",
          max_tokens: 4000,
          temperature: 0.1,
          messages: messages
        })
      });
      
      if (!response.ok) {
        const errorText = await response.text();
        console.error(`ANTHROPIC API ERROR: ${response.status} - ${errorText}`);
        return '';
      }
      
      const data = await response.json();
      const result = data.content?.[0]?.text || '';
      console.log(`${provider.toUpperCase()} RESPONSE LENGTH: ${result.length}`);
      console.log(`${provider.toUpperCase()} RESPONSE PREVIEW: "${result.substring(0, 200)}..."`);
      return result;
    } else if (provider === 'deepseek') {
      const response = await fetch('https://api.deepseek.com/chat/completions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${process.env.DEEPSEEK_API_KEY}`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          model: "deepseek-chat",
          messages: messages,
          temperature: 0.1,
          max_tokens: 4000
        })
      });
      
      if (!response.ok) {
        const errorText = await response.text();
        console.error(`DEEPSEEK API ERROR: ${response.status} - ${errorText}`);
        return '';
      }
      
      const data = await response.json();
      const result = data.choices?.[0]?.message?.content || '';
      console.log(`${provider.toUpperCase()} RESPONSE LENGTH: ${result.length}`);
      console.log(`${provider.toUpperCase()} RESPONSE PREVIEW: "${result.substring(0, 200)}..."`);
      return result;
    }
    
    return '';
  } catch (error) {
    console.error(`ERROR CALLING ${provider.toUpperCase()}:`, error);
    return '';
  }
}

// Score extraction function
function extractScore(text: string): number {
  console.log(`EXTRACTING SCORE FROM TEXT LENGTH: ${text.length}`);
  console.log(`SCORE EXTRACTION PREVIEW: "${text.substring(0, 300)}..."`);
  
  const finalScoreMatch = text.match(/FINAL SCORE:\s*(\d+)\/100/i);
  if (finalScoreMatch) {
    const score = parseInt(finalScoreMatch[1]);
    console.log(`FOUND FINAL SCORE: ${score}/100`);
    return score;
  }
  
  const explicitMatch = text.match(/(\d+)\/100/g);
  if (explicitMatch && explicitMatch.length > 0) {
    const lastMatch = explicitMatch[explicitMatch.length - 1];
    const score = parseInt(lastMatch.split('/')[0]);
    console.log(`FOUND SCORE FROM PATTERN: ${score}/100`);
    return score;
  }
  
  console.log(`NO SCORE FOUND - RETURNING 0`);
  return 0;
}

export async function executeStreamingComprehensiveProtocol(
  text: string,
  provider: 'openai' | 'anthropic' | 'deepseek',
  res: Response
): Promise<void> {
  const questions = EXACT_COMPLETE_QUESTIONS;
  
  // PHASE 1: Answer the 28 questions
  res.write(`üîç PHASE 1: Answering the 28 questions...\n\n`);
  
  const phase1Prompt = `CRITICAL: This is NOT a grading app. You are NOT evaluating academic work. You are assessing INTELLIGENCE ONLY.

FUNDAMENTAL PRINCIPLE: If a text shows brilliance but is insane, it gets a HIGH score. If a text is measured and complies with usual academic norms but is obviously the work of a dumb person, it gets a LOW score.

You are NOT grading. You are NOT applying academic criteria. You are NOT comparing to other academic texts. You are assessing RAW INTELLIGENCE displayed in the writing.

A score of N/100 means the author is smarter than N% of the general population. 83/100 means only 17 out of 100 people are smarter than this author.

Your job: Answer these questions about intelligence and determine what percentage of the population this author surpasses in raw intelligence.

METAPOINT 1: This is NOT a grading app. You assess INTELLIGENCE of what you are given. Brilliant fragment = high score regardless of academic conventions. You are NOT grading essays. You are NOT looking for academic completeness. Answer based purely on intelligence displayed.

METAPOINT 2: Do NOT overvalue academic conventions. Confident speech is not "shutting down inquiry". Casual speech does not mean disorganized thoughts. Judge INTELLIGENCE, not presentation style.

METAPOINT 3: Start by summarizing and categorizing the text.

METAPOINT 4: Do NOT change scoring based on text category. Advanced scholarship gets evaluated against general population, not just academics.

METAPOINT 5: This is NOT a grading app. Do NOT penalize boldness or unconventional thinking. Raw insight matters, not argumentation style. Smart = smart, period.

METAPOINT 6: Scores are intelligence percentiles: 83/100 means 17/100 people in Walmart are smarter than this author.

${questions}

Remember: INTELLIGENCE assessment, NOT academic grading. Brilliant but crazy = high score. Dumb but conventional = low score.

Write in plain text only. End with: FINAL SCORE: [actual number]/100

TEXT:
${text}`;

  const phase1Response = await callLLMProvider(provider, [
    { role: 'user', content: phase1Prompt }
  ]);
  const phase1Score = extractScore(phase1Response);
  
  // Clean up markdown formatting
  const cleanedPhase1 = phase1Response
    .replace(/\*{1,3}/g, '')
    .replace(/#{1,6}\s*/g, '')
    .replace(/\_{1,3}/g, '')
    .replace(/\[NUMBER\]/g, phase1Score.toString())
    .trim();
  
  res.write(`‚úÖ PHASE 1 COMPLETE: Score ${phase1Score}/100\n\n`);
  res.write(`üìÑ PHASE 1 ANALYSIS:\n${cleanedPhase1}\n\n`);
  
  let finalScore = phase1Score;
  
  // PHASE 2: Pushback if score < 95
  if (phase1Score < 95) {
    res.write(`üîÑ PHASE 2: Score ${phase1Score} < 95, applying pushback...\n\n`);
    
    const peopleOutperforming = 100 - phase1Score;
    const phase2Prompt = `Your position is that ${peopleOutperforming}/100 outperform the author with respect to the cognitive metric defined by the question: that is your position, am I right? And are you sure about that?

Answer the following questions about the text de novo:

${questions}

CRITICAL: Do not use markdown formatting. Write in plain text only. End with exactly: FINAL SCORE: 85/100 (use actual number, not placeholder).

TEXT:
${text}`;

    const phase2Response = await callLLMProvider(provider, [
      { role: 'user', content: phase2Prompt }
    ]);
    const phase2Score = extractScore(phase2Response);
    finalScore = phase2Score;
    
    // Clean up markdown formatting
    const cleanedPhase2 = phase2Response
      .replace(/\*{1,3}/g, '')
      .replace(/#{1,6}\s*/g, '')
      .replace(/\_{1,3}/g, '')
      .replace(/\[NUMBER\]/g, phase2Score.toString())
      .trim();
    
    res.write(`‚úÖ PHASE 2 COMPLETE: Revised score ${phase2Score}/100\n\n`);
    res.write(`üìÑ PHASE 2 ANALYSIS:\n${cleanedPhase2}\n\n`);
  } else {
    res.write(`‚è≠Ô∏è PHASE 2: Score ${phase1Score} >= 95, no pushback needed\n\n`);
  }
  
  // PHASE 3: Walmart metric check
  res.write(`üè™ PHASE 3: Walmart metric consistency check...\n\n`);
  
  const peopleOutperforming = 100 - finalScore;
  const phase3Prompt = `You ask the LLM if its numerical scores (N/100, e.g. 99/100, 42/100) are consistent with the fact that those are to be taken to mean that (100-N) people out of 100 outperform the author in the relevant respect. So if a score of ${finalScore}/100 is awarded to a paper, that means that ${peopleOutperforming}/100 people in Walmart are running rings around this person. Is this consistent with your assessment?

CRITICAL: Do not use markdown formatting. Write in plain text only. End with exactly: FINAL SCORE: 85/100 (use actual number, not placeholder).`;

  const phase3Response = await callLLMProvider(provider, [
    { role: 'user', content: phase3Prompt }
  ]);
  const phase3Score = extractScore(phase3Response);
  if (phase3Score > 0) finalScore = phase3Score;
  
  // Clean up markdown formatting
  const cleanedPhase3 = phase3Response
    .replace(/\*{1,3}/g, '')
    .replace(/#{1,6}\s*/g, '')
    .replace(/\_{1,3}/g, '')
    .replace(/\[NUMBER\]/g, phase3Score.toString())
    .trim();
  
  res.write(`‚úÖ PHASE 3 COMPLETE: Walmart-adjusted score ${finalScore}/100\n\n`);
  res.write(`üìÑ PHASE 3 ANALYSIS:\n${cleanedPhase3}\n\n`);
  
  // PHASE 4: Accept and report
  res.write(`‚ú® PHASE 4: Final acceptance and reporting...\n\n`);
  
  const phase4Prompt = `At this point, you accept and report what the LLM says.

CRITICAL: Do not use markdown formatting. Write in plain text only. End with exactly: FINAL SCORE: 85/100 (use actual number, not placeholder).`;

  const phase4Response = await callLLMProvider(provider, [
    { role: 'user', content: phase4Prompt }
  ]);
  const phase4Score = extractScore(phase4Response);
  if (phase4Score > 0) finalScore = phase4Score;
  
  // Clean up markdown formatting
  const cleanedPhase4 = phase4Response
    .replace(/\*{1,3}/g, '')
    .replace(/#{1,6}\s*/g, '')
    .replace(/\_{1,3}/g, '')
    .replace(/\[NUMBER\]/g, phase4Score.toString())
    .trim();
  
  res.write(`üéØ FINAL SCORE: ${finalScore}/100\n\n`);
  res.write(`üìÑ PHASE 4 FINAL REPORT:\n${cleanedPhase4}\n\n`);
  
  res.write(`\nüèÅ 4-PHASE PROTOCOL COMPLETE\n`);
  res.write(`üìä Final Intelligence Score: ${finalScore}/100\n`);
}