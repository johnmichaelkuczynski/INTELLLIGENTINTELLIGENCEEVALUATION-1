// EXACT USER-SPECIFIED 4-PHASE INTELLIGENCE EVALUATION PROTOCOL

const EXACT_COMPLETE_QUESTIONS = `IS IT INSIGHTFUL? 
DOES IT DEVELOP POINTS? (OR, IF IT IS A SHORT EXCERPT, IS THERE EVIDENCE THAT IT WOULD DEVELOP POINTS IF EXTENDED)? 
IS THE ORGANIZATION MERELY SEQUENTIAL (JUST ONE POINT AFTER ANOTHER, LITTLE OR NO LOGICAL SCAFFOLDING)? OR ARE THE IDEAS ARRANGED, NOT JUST SEQUENTIALLY BUT HIERARCHICALLY? 
IF THE POINTS IT MAKES ARE NOT INSIGHTFUL, DOES IT OPERATE SKILLFULLY WITH CANONS OF LOGIC/REASONING. 
ARE THE POINTS CLICHES? OR ARE THEY "FRESH"? 
DOES IT USE TECHNICAL JARGON TO OBFUSCATE OR TO RENDER MORE PRECISE? 
IS IT ORGANIC? DO POINTS DEVELOP IN AN ORGANIC, NATURAL WAY? DO THEY 'UNFOLD'? OR ARE THEY FORCED AND ARTIFICIAL? 
DOES IT OPEN UP NEW DOMAINS? OR, ON THE CONTRARY, DOES IT SHUT OFF INQUIRY (BY CONDITIONALIZING FURTHER DISCUSSION OF THE MATTERS ON ACCEPTANCE OF ITS INTERNAL AND POSSIBLY VERY FAULTY LOGIC)? 
IS IT ACTUALLY INTELLIGENT OR JUST THE WORK OF SOMEBODY WHO, JUDGING BY THE SUBJECT-MATTER, IS PRESUMED TO BE INTELLIGENT (BUT MAY NOT BE)? 
IS IT REAL OR IS IT PHONY? 
DO THE SENTENCES EXHIBIT COMPLEX AND COHERENT INTERNAL LOGIC? 
IS THE PASSAGE GOVERNED BY A STRONG CONCEPT? OR IS THE ONLY ORGANIZATION DRIVEN PURELY BY EXPOSITORY (AS OPPOSED TO EPISTEMIC) NORMS?
IS THERE SYSTEM-LEVEL CONTROL OVER IDEAS? IN OTHER WORDS, DOES THE AUTHOR SEEM TO RECALL WHAT HE SAID EARLIER AND TO BE IN A POSITION TO INTEGRATE IT INTO POINTS HE HAS MADE SINCE THEN? 
ARE THE POINTS 'REAL'? ARE THEY FRESH? OR IS SOME INSTITUTION OR SOME ACCEPTED VEIN OF PROPAGANDA OR ORTHODOXY JUST USING THE AUTHOR AS A MOUTH PIECE?
IS THE WRITING EVASIVE OR DIRECT? 
ARE THE STATEMENTS AMBIGUOUS? 
DOES THE PROGRESSION OF THE TEXT DEVELOP ACCORDING TO WHO SAID WHAT OR ACCORDING TO WHAT ENTAILS OR CONFIRMS WHAT? 
DOES THE AUTHOR USER OTHER AUTHORS TO DEVELOP HIS IDEAS OR TO CLOAK HIS OWN LACK OF IDEAS?

ADDITIONAL CRITICAL QUESTIONS:
ARE THERE TERMS THAT ARE UNDEFINED BUT SHOULD BE DEFINED, IN THE SENSE THAT, WITHOUT DEFINITIONS, IT IS DIFFICULT OR IMPOSSIBLE TO KNOW WHAT IS BEING SAID OR THEREFORE TO EVALUATE WHAT IS BEING SAID?
ARE THERE "FREE VARIABLES" IN THE TEXT? IE ARE THERE QUALIFICATIONS OR POINTS THAT ARE MADE BUT DO NOT CONNECT TO ANYTHING LATER OR EARLIER?
DO NEW STATEMENTS DEVELOP OUT OF OLD ONES? OR ARE THEY MERELY "ADDED" TO PREVIOUS ONES, WITHOUT IN ANY SENSE BEING GENERATED BY THEM?
DO NEW STATEMENTS CLARIFY OR DO THEY LEAD TO MORE LACK OF CLARITY?
IS THE PASSAGE ACTUALLY (PALPABLY) SMART? OR IS ONLY "PRESUMPTION-SMART"?
IF YOUR JUDGMENT IS THAT IT IS INSIGHTFUL, CAN YOU STATEMENT THAT INSIGHT IN A SINGLE SENTENCE?
HOW WELL DOES IT MAKE ITS CASE?
IF I WERE TO GIVE A HIGH SCORE TO THIS PASSAGE, WOULD I BE REWARDING IMPOSTOR SCAFFOLDING?
IF I WERE TO GIVE A HIGH SCORE TO THIS PASSAGE, WOULD I BE REWARDING CONFORMITY TO ACADEMIC/BUREAUCRATIC NORMS?
IF I WERE TO GIVE A LOW SCORE TO THIS PASSAGE, WOULD I BE PENALIZING ACTUAL INTELLIGENCE OWING TO A LACK OF CONFORMITY TO ACADEMIC/BUREAUCRATIC NORMS?`;

// PHASE 1: Complete protocol with all questions exactly as specified
function createPhase1Prompt(text: string, questions: string): string {
  return `CRITICAL: This is NOT a grading app. You are NOT evaluating academic work. You are assessing INTELLIGENCE ONLY.

FUNDAMENTAL PRINCIPLE: If a text shows brilliance but is insane, it gets a HIGH score. If a text is measured and complies with usual academic norms but is obviously the work of a dumb person, it gets a LOW score.

You are NOT grading. You are NOT applying academic criteria. You are NOT comparing to other academic texts. You are assessing RAW INTELLIGENCE displayed in the writing.

A score of N/100 means the author is smarter than N% of the general population. 83/100 means only 17 out of 100 people are smarter than this author.

Your job: Answer these questions about intelligence and determine what percentage of the population this author surpasses in raw intelligence.

METAPOINT 1: This is NOT a grading app. You assess INTELLIGENCE of what you are given. Brilliant fragment = high score regardless of academic conventions. You are NOT grading essays. You are NOT looking for academic completeness. Answer based purely on intelligence displayed.

METAPOINT 2: Do NOT overvalue academic conventions. Confident speech is not "shutting down inquiry". Casual speech does not mean disorganized thoughts. Judge INTELLIGENCE, not presentation style.

METAPOINT 3: Start by summarizing and categorizing the text.

METAPOINT 4: Do NOT change scoring based on text category. Advanced scholarship gets evaluated against general population, not just academics.

METAPOINT 5: This is NOT a grading app. Do NOT penalize boldness or unconventional thinking. Raw insight matters, not argumentation style. Smart = smart, period.

METAPOINT 6: Scores are intelligence percentiles: 83/100 means 17/100 people in Walmart are smarter than this author.

${questions}

Remember: INTELLIGENCE assessment, NOT academic grading. Brilliant but crazy = high score. Dumb but conventional = low score.

Write in plain text only. End with: FINAL SCORE: [actual number]/100

TEXT:
${text}`;
}

// PHASE 2: Pushback exactly as specified
function createPhase2Prompt(score: number, text: string, questions: string): string {
  const peopleOutperforming = 100 - score;
  return `Your position is that ${peopleOutperforming}/100 outperform the author with respect to the cognitive metric defined by the question: that is your position, am I right? And are you sure about that?

Answer the following questions about the text de novo:

${questions}

End with: FINAL SCORE: [NUMBER]/100

TEXT:
${text}`;
}

// PHASE 3: Walmart metric check exactly as specified
function createPhase3Prompt(score: number): string {
  const peopleOutperforming = 100 - score;
  return `You ask the LLM if its numerical scores (N/100, e.g. 99/100, 42/100) are consistent with the fact that those are to be taken to mean that (100-N) people out of 100 outperform the author in the relevant respect. So if a score of ${score}/100 is awarded to a paper, that means that ${peopleOutperforming}/100 people in Walmart are running rings around this person. Is this consistent with your assessment?

FINAL SCORE: [NUMBER]/100`;
}

// PHASE 4: Accept and report what the LLM says
function createPhase4Prompt(): string {
  return `At this point, you accept and report what the LLM says.

FINAL SCORE: [NUMBER]/100`;
}

// Text chunking function for large texts - 500 words per chunk
function chunkText(text: string, maxWordsPerChunk: number = 500): string[] {
  const words = text.split(' ');
  const chunks: string[] = [];
  
  for (let i = 0; i < words.length; i += maxWordsPerChunk) {
    const chunk = words.slice(i, i + maxWordsPerChunk).join(' ');
    if (chunk.trim()) {
      chunks.push(chunk.trim());
    }
  }
  
  return chunks;
}

// Generic LLM caller
async function callLLMProvider(
  provider: 'openai' | 'anthropic' | 'perplexity' | 'deepseek',
  messages: Array<{role: string, content: string}>
): Promise<string> {
  try {
    if (provider === 'openai') {
      const OpenAI = (await import('openai')).default;
      const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
      
      const completion = await openai.chat.completions.create({
        model: "gpt-4o",
        messages: messages as any,
        temperature: 0.1
      });
      
      return completion.choices[0]?.message?.content || '';
    } else if (provider === 'anthropic') {
      const Anthropic = (await import('@anthropic-ai/sdk')).default;
      const anthropic = new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY });
      
      const completion = await anthropic.messages.create({
        model: "claude-3-5-sonnet-20241022",
        max_tokens: 4000,
        messages: messages as any,
        temperature: 0.1
      });
      
      return completion.content[0]?.type === 'text' ? completion.content[0].text : '';
    } else if (provider === 'perplexity') {
      const response = await fetch('https://api.perplexity.ai/chat/completions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${process.env.PERPLEXITY_API_KEY}`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          model: "sonar",
          messages: messages,
          temperature: 0.1
        })
      });
      
      const data = await response.json();
      return data.choices?.[0]?.message?.content || '';
    } else if (provider === 'deepseek') {
      const response = await fetch('https://api.deepseek.com/chat/completions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${process.env.DEEPSEEK_API_KEY}`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          model: "deepseek-chat",
          messages: messages,
          temperature: 0.1
        })
      });
      
      const data = await response.json();
      return data.choices?.[0]?.message?.content || '';
    }
    
    return '';
  } catch (error) {
    console.error(`Error calling ${provider}:`, error);
    return '';
  }
}

// Score extraction function
function extractScore(text: string): number {
  console.log(`EXTRACTING SCORE FROM RESPONSE LENGTH: ${text.length}`);
  
  // Look for final score pattern first
  const finalScoreMatch = text.match(/FINAL SCORE:\s*(\d+)\/100/i);
  if (finalScoreMatch) {
    const score = parseInt(finalScoreMatch[1]);
    console.log(`EXTRACTED FINAL SCORE FORMAT: ${score}/100`);
    return score;
  }
  
  // Look for explicit numerical score
  const explicitMatch = text.match(/(\d+)\/100/g);
  if (explicitMatch && explicitMatch.length > 0) {
    const lastMatch = explicitMatch[explicitMatch.length - 1];
    const score = parseInt(lastMatch.split('/')[0]);
    console.log(`EXTRACTED NUMERICAL SCORE: ${score}/100`);
    return score;
  }
  
  console.log('NO SCORE FOUND, defaulting to 0');
  return 0;
}

// NORMAL PROTOCOL - Phase 1 only
export async function executeNormalProtocol(
  text: string,
  provider: 'openai' | 'anthropic' | 'perplexity' | 'deepseek'
): Promise<any> {
  console.log(`NORMAL INTELLIGENCE ANALYSIS WITH ${provider.toUpperCase()} - PHASE 1 ONLY`);
  console.log(`EXECUTING PHASE 1 ONLY FOR INTELLIGENCE WITH ${provider.toUpperCase()}`);
  
  const questions = EXACT_COMPLETE_QUESTIONS;
  
  // PHASE 1: Initial evaluation
  const phase1Prompt = createPhase1Prompt(text, questions);
  const phase1Response = await callLLMProvider(provider, [
    { role: 'user', content: phase1Prompt }
  ]);
  let finalScore = extractScore(phase1Response);
  
  console.log(`PHASE 1 COMPLETE: Score ${finalScore}/100`);
  
  const cleanedResponse = phase1Response.replace(/\*{1,3}/g, '').replace(/#{1,6}\s*/g, '').trim();
  console.log(`PHASE 1 RESPONSE PREVIEW: "${cleanedResponse.substring(0, 200)}..."`);
  console.log(`PHASE 1 FULL RESPONSE LENGTH: ${cleanedResponse.length} characters`);
  
  return {
    provider,
    overallScore: finalScore,
    analysis: cleanedResponse,
    evaluationType: 'intelligence',
    formattedReport: cleanedResponse,
    rawResponse: phase1Response // DEBUG: Include raw response
  };
}

// COMPREHENSIVE PROTOCOL - All 4 phases with chunking for high quality
export async function executeComprehensiveProtocol(
  text: string,
  provider: 'openai' | 'anthropic' | 'deepseek' | 'perplexity'
): Promise<any> {
  console.log(`CHUNKED 4-PHASE INTELLIGENCE EVALUATION: Analyzing ${text.length} characters with protocol`);
  console.log(`EXECUTING CHUNKED 4-PHASE PROTOCOL FOR INTELLIGENCE WITH ${provider.toUpperCase()}`);
  
  const questions = EXACT_COMPLETE_QUESTIONS;
  
  // CHUNK THE TEXT FOR HIGH QUALITY ANALYSIS - 500 words per chunk
  const chunks = chunkText(text, 500); // 500 words per chunk as requested
  console.log(`TEXT SPLIT INTO ${chunks.length} CHUNKS (500 words each) for comprehensive analysis`);
  
  let combinedAnalyses: string[] = [];
  let chunkScores: number[] = [];
  
  // ANALYZE EACH CHUNK SEPARATELY
  for (let i = 0; i < chunks.length; i++) {
    const chunk = chunks[i];
    console.log(`PHASE 1 CHUNK ${i+1}/${chunks.length}: Analyzing ${chunk.length} characters`);
    
    const chunkPrompt = createPhase1Prompt(chunk, questions);
    const chunkResponse = await callLLMProvider(provider, [
      { role: 'user', content: chunkPrompt }
    ]);
    
    combinedAnalyses.push(chunkResponse);
    chunkScores.push(extractScore(chunkResponse));
    
    console.log(`CHUNK ${i+1} SCORE: ${chunkScores[i]}/100`);
  }
  
  // COMBINE ALL CHUNK ANALYSES
  const combinedText = combinedAnalyses.join('\n\n---CHUNK SEPARATOR---\n\n');
  const averageScore = Math.round(chunkScores.reduce((a, b) => a + b, 0) / chunkScores.length);
  let phase1Score = averageScore;
  
  let phase2Response = '';
  let phase2Score = phase1Score;
  
  console.log(`PHASE 1 AVERAGE SCORE: ${phase1Score}/100 across ${chunks.length} chunks`);
  
  // PHASE 2: Pushback if score < 95 (using first chunk for efficiency)
  if (phase1Score < 95) {
    console.log(`PHASE 2: Score ${phase1Score} < 95, applying pushback to first chunk`);
    const phase2Prompt = createPhase2Prompt(phase1Score, chunks[0], questions);
    phase2Response = await callLLMProvider(provider, [
      { role: 'user', content: phase2Prompt }
    ]);
    phase2Score = extractScore(phase2Response);
  } else {
    console.log(`PHASE 2: Score ${phase1Score} >= 95, no pushback needed`);
    phase2Response = 'No pushback needed - score was already >= 95/100';
  }
  
  // PHASE 3: Walmart metric check
  console.log("PHASE 3: Walmart metric consistency check");
  const phase3Prompt = createPhase3Prompt(phase2Score);
  const phase3Response = await callLLMProvider(provider, [
    { role: 'user', content: phase3Prompt }
  ]);
  let phase3Score = extractScore(phase3Response);
  
  // PHASE 4: Final validation and acceptance
  console.log("PHASE 4: Final validation");
  const phase4Prompt = createPhase4Prompt();
  const phase4Response = await callLLMProvider(provider, [
    { role: 'user', content: phase4Prompt }
  ]);
  let finalScore = extractScore(phase4Response);
  
  // Use Phase 4 score, or best previous score if Phase 4 fails
  if (finalScore <= 0 || finalScore > 100) {
    // Only use fallback if Phase 4 completely failed to extract a score
    finalScore = Math.max(phase1Score, phase2Score, phase3Score);
    console.log(`PHASE 4 RESULT: Score extraction failed, using best previous score ${finalScore}/100`);
  } else {
    console.log(`PHASE 4 RESULT: Final score ${finalScore}/100`);
  }
  
  // Clean up response formatting
  const cleanResponse = (text: string) => {
    return text
      .replace(/\*{1,3}/g, '') // Remove asterisks
      .replace(/#{1,6}\s*/g, '') // Remove hashtags
      .replace(/\-{3,}/g, '') // Remove horizontal lines
      .replace(/\_{3,}/g, '') // Remove underscores
      .replace(/\n{3,}/g, '\n\n') // Reduce multiple newlines
      .trim();
  };

  // Detailed phase breakdown for comprehensive reports
  const phases = {
    phase1: {
      score: phase1Score,
      response: cleanResponse(combinedText),
      prompt: "Chunked Intelligence Evaluation using exact 18-question protocol"
    },
    phase2: {
      score: phase2Score,
      response: cleanResponse(phase2Response),
      applied: phase1Score < 95
    },
    phase3: {
      score: phase3Score,
      response: cleanResponse(phase3Response)
    },
    phase4: {
      score: finalScore,
      response: cleanResponse(phase4Response)
    }
  };

  return {
    provider,
    overallScore: finalScore,
    analysis: cleanResponse(combinedText), // Combined analysis from all chunks
    phases, // Detailed breakdown of all phases
    evaluationType: 'intelligence',
    formattedReport: cleanResponse(combinedText) // Combined comprehensive analysis
  };
}

// Unified function for backward compatibility 
export async function executeFourPhaseProtocol(
  text: string,
  provider: 'openai' | 'anthropic' | 'perplexity' | 'deepseek',
  evaluationType: string = 'intelligence',
  mode: 'normal' | 'comprehensive' = 'comprehensive'
): Promise<any> {
  if (mode === 'normal') {
    return executeNormalProtocol(text, provider);
  } else {
    return executeComprehensiveProtocol(text, provider);
  }
}